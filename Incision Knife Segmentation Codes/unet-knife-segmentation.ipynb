{"cells":[{"cell_type":"code","execution_count":1,"id":"C5Po_W8YY3Da","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20985,"status":"ok","timestamp":1732822610208,"user":{"displayName":"Capstone Jesus","userId":"09341695596999976336"},"user_tz":-330},"id":"C5Po_W8YY3Da","outputId":"ab065523-c86a-498c-934c-e473e67f56e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"id":"6feb0d80","metadata":{"executionInfo":{"elapsed":2978,"status":"ok","timestamp":1732822613181,"user":{"displayName":"Capstone Jesus","userId":"09341695596999976336"},"user_tz":-330},"id":"6feb0d80"},"outputs":[],"source":["#boxfilter.py\n","import torch\n","from torch import nn\n","\n","def diff_x(input, r):\n","    assert input.dim() == 4\n","\n","    left   = input[:, :,         r:2 * r + 1]\n","    middle = input[:, :, 2 * r + 1:         ] - input[:, :,           :-2 * r - 1]\n","    right  = input[:, :,        -1:         ] - input[:, :, -2 * r - 1:    -r - 1]\n","\n","    output = torch.cat([left, middle, right], dim=2)\n","\n","    return output\n","\n","def diff_y(input, r):\n","    assert input.dim() == 4\n","\n","    left   = input[:, :, :,         r:2 * r + 1]\n","    middle = input[:, :, :, 2 * r + 1:         ] - input[:, :, :,           :-2 * r - 1]\n","    right  = input[:, :, :,        -1:         ] - input[:, :, :, -2 * r - 1:    -r - 1]\n","\n","    output = torch.cat([left, middle, right], dim=3)\n","    return output\n","\n","#\n","class BoxFilter(nn.Module):\n","    def __init__(self, r):\n","        super(BoxFilter, self).__init__()\n","        self.r = r\n","\n","    def forward(self, x):\n","        assert x.dim() == 4\n","\n","        return diff_y(diff_x(x.cumsum(dim=2), self.r).cumsum(dim=3), self.r)"]},{"cell_type":"code","execution_count":3,"id":"90e175d2","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1732822613182,"user":{"displayName":"Capstone Jesus","userId":"09341695596999976336"},"user_tz":-330},"id":"90e175d2"},"outputs":[],"source":["#fastguidedfilter.py\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.autograd import Variable\n","import torch\n","\n","class GuidedFilter(nn.Module):\n","    def __init__(self, r, eps=1e-8):\n","        super(GuidedFilter, self).__init__()\n","\n","        self.r = r\n","        self.eps = eps\n","        self.boxfilter = BoxFilter(r)\n","\n","\n","    def forward(self, x, y):\n","        n_x, c_x, h_x, w_x = x.size()\n","        n_y, c_y, h_y, w_y = y.size()\n","\n","        assert n_x == n_y\n","        assert c_x == 1 or c_x == c_y\n","        assert h_x == h_y and w_x == w_y\n","        assert h_x \u003e 2 * self.r + 1 and w_x \u003e 2 * self.r + 1\n","\n","        # N\n","        N = self.boxfilter(Variable(x.data.new().resize_((1, 1, h_x, w_x)).fill_(1.0)))\n","\n","        # mean_x\n","        mean_x = self.boxfilter(x) / N\n","        # mean_y\n","        mean_y = self.boxfilter(y) / N\n","\n","        # cov_xy\n","        cov_xy = self.boxfilter(x * y) / N - mean_x * mean_y\n","        # var_x\n","        var_x = self.boxfilter(x * x) / N - mean_x * mean_x\n","\n","        # A\n","        A = cov_xy / (var_x + self.eps)\n","        # b\n","        b = mean_y - A * mean_x\n","\n","        # mean_A; mean_b\n","        mean_A = self.boxfilter(A) / N\n","        mean_b = self.boxfilter(b) / N\n","\n","        return (mean_A * x + mean_b).float()\n"]},{"cell_type":"code","execution_count":4,"id":"accba400","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3484,"status":"ok","timestamp":1732822616659,"user":{"displayName":"Capstone Jesus","userId":"09341695596999976336"},"user_tz":-330},"id":"accba400","outputId":"c3e00343-b73b-4dfa-ff25-d700855e4317"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 1, 512, 512])\n"]}],"source":["#model.py\n","import torch\n","import torch.nn as nn\n","\n","class conv_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_c)\n","\n","        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_c)\n","\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, inputs):\n","        x = self.conv1(inputs)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        return x\n","\n","class encoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.conv = conv_block(in_c, out_c)\n","        self.pool = nn.MaxPool2d((2, 2))\n","\n","    def forward(self, inputs):\n","        x = self.conv(inputs)\n","        p = self.pool(x)\n","\n","        return x, p\n","\n","class decoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n","        self.conv = conv_block(out_c+out_c, out_c)\n","\n","    def forward(self, inputs, skip):\n","        x = self.up(inputs)\n","        x = torch.cat([x, skip], axis=1)\n","        x = self.conv(x)\n","        return x\n","\n","class build_unet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        \"\"\" Encoder \"\"\"\n","        self.e1 = encoder_block(3, 64)\n","        self.e2 = encoder_block(64, 128)\n","        self.e3 = encoder_block(128, 256)\n","        self.e4 = encoder_block(256, 512)\n","\n","        \"\"\" Bottleneck \"\"\"\n","        self.b = conv_block(512, 1024)\n","\n","        \"\"\" Decoder \"\"\"\n","        self.d1 = decoder_block(1024, 512)\n","        self.d2 = decoder_block(512, 256)\n","        self.d3 = decoder_block(256, 128)\n","        self.d4 = decoder_block(128, 64)\n","\n","        self.gf = GuidedFilter(r=2, eps=1e-2)\n","\n","        \"\"\" Classifier \"\"\"\n","        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n","\n","    def forward(self, inputs):\n","        \"\"\" Encoder \"\"\"\n","        s1, p1 = self.e1(inputs)\n","        gray_inputs1 = 0.299 * p1[:, 0, :, :] + 0.587 * p1[:, 1, :, :] + 0.114 * p1[:, 2, :, :]\n","        p1 = self.gf(gray_inputs1.unsqueeze(1), p1)\n","        s2, p2 = self.e2(p1)\n","        s3, p3 = self.e3(p2)\n","        s4, p4 = self.e4(p3)\n","\n","        \"\"\" Bottleneck \"\"\"\n","        b = self.b(p4)\n","\n","        \"\"\" Decoder \"\"\"\n","        d1 = self.d1(b, s4)\n","        d2 = self.d2(d1, s3)\n","        d3 = self.d3(d2, s2)\n","        d4 = self.d4(d3, s1)\n","\n","        outputs = self.outputs(d4)\n","\n","        return outputs\n","\n","if __name__ == \"__main__\":\n","    x = torch.randn((2, 3, 512, 512))\n","    f = build_unet()\n","    y = f(x)\n","    print(y.shape)\n"]},{"cell_type":"code","execution_count":5,"id":"1c2fe3e0","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1732822616660,"user":{"displayName":"Capstone Jesus","userId":"09341695596999976336"},"user_tz":-330},"id":"1c2fe3e0"},"outputs":[],"source":["#data.py\n","import os\n","import numpy as np\n","import cv2\n","import torch\n","from torch.utils.data import Dataset\n","\n","W = 512\n","H = 512\n","size = (W,H)\n","class DriveDataset(Dataset):\n","    def __init__(self, images_path, masks_path):\n","\n","        self.images_path = images_path\n","        self.masks_path = masks_path\n","        self.n_samples = len(images_path)\n","\n","    def __getitem__(self, index):\n","        \"\"\" Reading image \"\"\"\n","        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n","        image = cv2.resize(image, size)\n","        image = image/255.0 ## (512, 512, 3)\n","        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n","        image = image.astype(np.float32)\n","        image = torch.from_numpy(image)\n","\n","        \"\"\" Reading mask \"\"\"\n","        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n","        mask = cv2.resize(mask, size)\n","        mask = mask/255.0   ## (512, 512)\n","        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)\n","        mask = mask.astype(np.float32)\n","        mask = torch.from_numpy(mask)\n","\n","        return image, mask\n","\n","    def __len__(self):\n","        return self.n_samples\n"]},{"cell_type":"code","execution_count":6,"id":"51e66a0f","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1732822616660,"user":{"displayName":"Capstone Jesus","userId":"09341695596999976336"},"user_tz":-330},"id":"51e66a0f"},"outputs":[],"source":["#loss.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","\n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = torch.sigmoid(inputs)\n","\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","\n","        return 1 - dice\n","\n","class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","\n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = torch.sigmoid(inputs)\n","\n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        Dice_BCE = BCE + dice_loss\n","\n","        return Dice_BCE\n"]},{"cell_type":"code","execution_count":7,"id":"cb09c8dd","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1732822616661,"user":{"displayName":"Capstone Jesus","userId":"09341695596999976336"},"user_tz":-330},"id":"cb09c8dd"},"outputs":[],"source":["#utils.py\n","import os\n","import time\n","import random\n","import numpy as np\n","import cv2\n","import torch\n","\n","\"\"\" Seeding the randomness. \"\"\"\n","def seeding(seed):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\"\"\" Create a directory. \"\"\"\n","def create_dir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","\"\"\" Calculate the time taken \"\"\"\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n"]},{"cell_type":"code","execution_count":null,"id":"9519f90b","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"9519f90b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Size:\n","Train: 192 - Valid: 32\n","\n","build_unet(\n","  (e1): encoder_block(\n","    (conv): conv_block(\n","      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (e2): encoder_block(\n","    (conv): conv_block(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (e3): encoder_block(\n","    (conv): conv_block(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (e4): encoder_block(\n","    (conv): conv_block(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (b): conv_block(\n","    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU()\n","  )\n","  (d1): decoder_block(\n","    (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n","    (conv): conv_block(\n","      (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","  )\n","  (d2): decoder_block(\n","    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n","    (conv): conv_block(\n","      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","  )\n","  (d3): decoder_block(\n","    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n","    (conv): conv_block(\n","      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","  )\n","  (d4): decoder_block(\n","    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n","    (conv): conv_block(\n","      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","  )\n","  (gf): GuidedFilter(\n","    (boxfilter): BoxFilter()\n","  )\n","  (outputs): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",")\n","Number of trainable parameters: 31043521\n","Valid loss improved from inf to 1.1110. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 01 | Epoch Time: 0m 33s\n","\tTrain Loss: 1.177\n","\t Val. Loss: 1.111\n","\n","Valid loss improved from 1.1110 to 1.0516. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 02 | Epoch Time: 0m 33s\n","\tTrain Loss: 1.037\n","\t Val. Loss: 1.052\n","\n","Valid loss improved from 1.0516 to 0.9530. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 03 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.962\n","\t Val. Loss: 0.953\n","\n","Valid loss improved from 0.9530 to 0.8975. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 04 | Epoch Time: 0m 33s\n","\tTrain Loss: 0.890\n","\t Val. Loss: 0.897\n","\n","Valid loss improved from 0.8975 to 0.8725. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 05 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.815\n","\t Val. Loss: 0.873\n","\n","Valid loss improved from 0.8725 to 0.7486. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 06 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.741\n","\t Val. Loss: 0.749\n","\n","Valid loss improved from 0.7486 to 0.7013. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 07 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.669\n","\t Val. Loss: 0.701\n","\n","Valid loss improved from 0.7013 to 0.6297. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 08 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.586\n","\t Val. Loss: 0.630\n","\n","Valid loss improved from 0.6297 to 0.5463. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 09 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.502\n","\t Val. Loss: 0.546\n","\n","Valid loss improved from 0.5463 to 0.5068. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 10 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.429\n","\t Val. Loss: 0.507\n","\n","Epoch: 11 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.360\n","\t Val. Loss: 0.589\n","\n","Valid loss improved from 0.5068 to 0.4970. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 12 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.339\n","\t Val. Loss: 0.497\n","\n","Valid loss improved from 0.4970 to 0.3237. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 13 | Epoch Time: 0m 33s\n","\tTrain Loss: 0.258\n","\t Val. Loss: 0.324\n","\n","Epoch: 14 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.214\n","\t Val. Loss: 0.350\n","\n","Valid loss improved from 0.3237 to 0.2534. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 15 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.186\n","\t Val. Loss: 0.253\n","\n","Valid loss improved from 0.2534 to 0.2350. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 16 | Epoch Time: 0m 33s\n","\tTrain Loss: 0.159\n","\t Val. Loss: 0.235\n","\n","Valid loss improved from 0.2350 to 0.2019. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 17 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.136\n","\t Val. Loss: 0.202\n","\n","Epoch: 18 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.117\n","\t Val. Loss: 0.212\n","\n","Valid loss improved from 0.2019 to 0.1934. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 19 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.103\n","\t Val. Loss: 0.193\n","\n","Epoch: 20 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.091\n","\t Val. Loss: 0.223\n","\n","Epoch: 21 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.080\n","\t Val. Loss: 0.256\n","\n","Valid loss improved from 0.1934 to 0.1590. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 22 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.075\n","\t Val. Loss: 0.159\n","\n","Epoch: 23 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.073\n","\t Val. Loss: 0.244\n","\n","Valid loss improved from 0.1590 to 0.1498. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 24 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.074\n","\t Val. Loss: 0.150\n","\n","Epoch: 25 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.104\n","\t Val. Loss: 0.189\n","\n","Epoch: 26 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.067\n","\t Val. Loss: 0.229\n","\n","Epoch: 27 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.062\n","\t Val. Loss: 0.204\n","\n","Epoch: 28 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.055\n","\t Val. Loss: 0.209\n","\n","Epoch: 29 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.046\n","\t Val. Loss: 0.181\n","\n","Epoch: 30 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.044\n","\t Val. Loss: 0.192\n","\n","Valid loss improved from 0.1498 to 0.1468. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 31 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.040\n","\t Val. Loss: 0.147\n","\n","Epoch: 32 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.038\n","\t Val. Loss: 0.171\n","\n","Epoch: 33 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.035\n","\t Val. Loss: 0.163\n","\n","Epoch: 34 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.033\n","\t Val. Loss: 0.165\n","\n","Epoch: 35 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.032\n","\t Val. Loss: 0.150\n","\n","Epoch: 36 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.030\n","\t Val. Loss: 0.156\n","\n","Epoch: 37 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.030\n","\t Val. Loss: 0.155\n","\n","Valid loss improved from 0.1468 to 0.1464. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 38 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.028\n","\t Val. Loss: 0.146\n","\n","Epoch: 39 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.028\n","\t Val. Loss: 0.154\n","\n","Epoch: 40 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.027\n","\t Val. Loss: 0.150\n","\n","Valid loss improved from 0.1464 to 0.1416. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 41 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.025\n","\t Val. Loss: 0.142\n","\n","Epoch: 42 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.025\n","\t Val. Loss: 0.152\n","\n","Epoch: 43 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.024\n","\t Val. Loss: 0.143\n","\n","Epoch: 44 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.023\n","\t Val. Loss: 0.158\n","\n","Epoch: 45 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.022\n","\t Val. Loss: 0.169\n","\n","Valid loss improved from 0.1416 to 0.1406. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 46 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.023\n","\t Val. Loss: 0.141\n","\n","Valid loss improved from 0.1406 to 0.1209. Saving checkpoint: /content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\n","Epoch: 47 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.021\n","\t Val. Loss: 0.121\n","\n","Epoch: 48 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.021\n","\t Val. Loss: 0.130\n","\n","Epoch: 49 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.020\n","\t Val. Loss: 0.127\n","\n","Epoch: 50 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.020\n","\t Val. Loss: 0.129\n","\n"]}],"source":["#train.py\n","import os\n","import time\n","from glob import glob\n","\n","import torch\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","\n","def train(model, loader, optimizer, loss_fn, device):\n","    epoch_loss = 0.0\n","\n","    model.train()\n","    for x, y in loader:\n","        x = x.to(device, dtype=torch.float32)\n","        y = y.to(device, dtype=torch.float32)\n","\n","        optimizer.zero_grad()\n","        y_pred = model(x)\n","        loss = loss_fn(y_pred, y)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    epoch_loss = epoch_loss/len(loader)\n","    return epoch_loss\n","\n","def evaluate(model, loader, loss_fn, device):\n","    epoch_loss = 0.0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device, dtype=torch.float32)\n","            y = y.to(device, dtype=torch.float32)\n","\n","            y_pred = model(x)\n","            loss = loss_fn(y_pred, y)\n","            epoch_loss += loss.item()\n","\n","        epoch_loss = epoch_loss/len(loader)\n","    return epoch_loss\n","\n","if __name__ == \"__main__\":\n","    \"\"\" Seeding \"\"\"\n","    seeding(42)\n","\n","    \"\"\" Directories \"\"\"\n","    create_dir(\"files\")\n","\n","    \"\"\" Load dataset and checkpoint path \"\"\"\n","    train_x = sorted(glob(\"/content/drive/MyDrive/Capstone/Training Incision Segmentation/Knife Dataset/train imgs/*\"))\n","    train_y = sorted(glob(\"/content/drive/MyDrive/Capstone/Training Incision Segmentation/Knife Dataset/train masks/*\"))\n","    valid_x = sorted(glob(\"/content/drive/MyDrive/Capstone/Training Incision Segmentation/Knife Dataset/val imgs/*\"))\n","    valid_y = sorted(glob(\"/content/drive/MyDrive/Capstone/Training Incision Segmentation/Knife Dataset/val masks/*\"))\n","    checkpoint_path = \"/content/drive/MyDrive/Capstone/Training Incision Segmentation/UNet/checkpoint_gf_50epochs_l4.pth\"\n","\n","    data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n","    print(data_str)\n","\n","    \"\"\" Hyperparameters \"\"\"\n","    H = 512\n","    W = 512\n","    size = (H, W)\n","    batch_size = 2\n","    num_epochs = 50\n","    lr = 1e-4\n","\n","\n","    \"\"\" Dataset and loader \"\"\"\n","    train_dataset = DriveDataset(train_x, train_y)\n","    valid_dataset = DriveDataset(valid_x, valid_y)\n","\n","    train_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=2\n","    )\n","\n","    valid_loader = DataLoader(\n","        dataset=valid_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","\n","    device = torch.device('cuda')\n","    model = build_unet()\n","    model = model.to(device)\n","\n","    print(model)\n","    print(\"Number of trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n","    loss_fn = DiceBCELoss()\n","\n","    \"\"\" Training the model \"\"\"\n","    best_valid_loss = float(\"inf\")\n","\n","    for epoch in range(num_epochs):\n","        start_time = time.time()\n","\n","        train_loss = train(model, train_loader, optimizer, loss_fn, device)\n","        valid_loss = evaluate(model, valid_loader, loss_fn, device)\n","\n","        \"\"\" Saving the model \"\"\"\n","        if valid_loss \u003c best_valid_loss:\n","            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n","            print(data_str)\n","\n","            best_valid_loss = valid_loss\n","            torch.save(model.state_dict(), checkpoint_path)\n","\n","        end_time = time.time()\n","        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n","        data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n","        data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n","        print(data_str)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","name":"","version":""},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6066668,"sourceId":9880657,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"papermill":{"default_parameters":{},"duration":11798.736354,"end_time":"2024-11-14T21:53:49.608433","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-11-14T18:37:10.872079","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}